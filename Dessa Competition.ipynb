{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "## load required libraries \n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import eli5\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Campaigns:  378661 \n",
      "Number of Columns:  15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1533.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000003930</td>\n",
       "      <td>Greeting From Earth: ZGAC Arts Capsule For ET</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>30000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>45000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>19500.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               name  \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000003930      Greeting From Earth: ZGAC Arts Capsule For ET   \n",
       "2  1000004038                                     Where is Hank?   \n",
       "3  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "4  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "\n",
       "         category main_category currency   deadline     goal  \\\n",
       "0          Poetry    Publishing      GBP 2015-10-09   1000.0   \n",
       "1  Narrative Film  Film & Video      USD 2017-11-01  30000.0   \n",
       "2  Narrative Film  Film & Video      USD 2013-02-26  45000.0   \n",
       "3           Music         Music      USD 2012-04-16   5000.0   \n",
       "4    Film & Video  Film & Video      USD 2015-08-29  19500.0   \n",
       "\n",
       "             launched  pledged     state  backers country  usd pledged  \\\n",
       "0 2015-08-11 12:12:28      0.0    failed        0      GB          0.0   \n",
       "1 2017-09-02 04:43:57   2421.0    failed       15      US        100.0   \n",
       "2 2013-01-12 00:20:50    220.0    failed        3      US        220.0   \n",
       "3 2012-03-17 03:24:11      1.0    failed        1      US          1.0   \n",
       "4 2015-07-04 08:35:03   1283.0  canceled       14      US       1283.0   \n",
       "\n",
       "   usd_pledged_real  usd_goal_real  \n",
       "0               0.0        1533.95  \n",
       "1            2421.0       30000.00  \n",
       "2             220.0       45000.00  \n",
       "3               1.0        5000.00  \n",
       "4            1283.0       19500.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv(\"../input/ks-projects-201801.csv\",  parse_dates = [\"launched\", \"deadline\"])\n",
    "df_copy = df.copy()  #keep a copy of the data frame that remains intact\n",
    "print (\"Number of Campaigns: \", df.shape[0], \"\\nNumber of Columns: \", df.shape[1])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of every machine learning problem, we need to explore the data and look out for problematic cases. These steps can be listed below:\n",
    "* Outliers\n",
    "* Missing values\n",
    "* Unwanted information/labels\n",
    "* Encode categorical values to numerical\n",
    "* Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there are any missing values. We will deal with outliers by considering features relevant to mean and median of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usd pledged         3797\n",
       "name                   4\n",
       "usd_goal_real          0\n",
       "usd_pledged_real       0\n",
       "country                0\n",
       "backers                0\n",
       "state                  0\n",
       "pledged                0\n",
       "launched               0\n",
       "goal                   0\n",
       "deadline               0\n",
       "currency               0\n",
       "main_category          0\n",
       "category               0\n",
       "ID                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as it shows, usd_pledged has around 1% of it's values missing and there are 4 campaigns with missing names. Let's take a further look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, name, category, main_category, currency, deadline, goal, launched, pledged, state, backers, country, usd pledged, usd_pledged_real, usd_goal_real]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df[df['usd pledged'].isnull()].shape)\n",
    "df[df['usd pledged'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems reasonable to keep these 3797 rows and substitute the NAN values with the corresponding usd_pledged_real and drop those 4 rows with no name. \n",
    "Also a glance at the country column shows that some country entries seem to be incorrect with the '*N,0\"*' entry. We should also consider taking care of that. \n",
    "Since most of the campaigns are from the states (78.85%) we will use a indicator of being from the states or not. Hence these values will turn to 0 (not from the states, assuming that '*N,0\"*' does not indicate that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['usd pledged'].fillna(df['usd_pledged_real'], inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usd_goal_real       0\n",
       "usd_pledged_real    0\n",
       "usd pledged         0\n",
       "country             0\n",
       "backers             0\n",
       "state               0\n",
       "pledged             0\n",
       "launched            0\n",
       "goal                0\n",
       "deadline            0\n",
       "currency            0\n",
       "main_category       0\n",
       "category            0\n",
       "name                0\n",
       "ID                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's make sure the null values have been taken care of:\n",
    "df.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now that we have dealt with the missing values, let's delve into the feature extraction step. Below is a list of useful features that we will be using:\n",
    "*Features focusing on the name of the campaign can be:*\n",
    "* Number of words in the name\n",
    "* Number of syllables\n",
    "* Presence of particular punctuation (e.g. ? !)\n",
    "* Number of characters\n",
    "* Ratio between vowels and alphanumeric length of the name\n",
    "* Whether the name is uppercase or not (Maybe names in uppercase can attract more attention to potential backers)\n",
    "\n",
    "These features are useful because we are interested to know whether a specific range of words, characters or any sort of pattern is effective in the campaign's success.\n",
    "Other important features could be those relevant to the time of launch and its durations. There may be a correlation between the time and day/month a campaign is launched.\n",
    "Some features in this aspect could be:\n",
    "* Launched hour\n",
    "* Launched day\n",
    "* Launched week\n",
    "* Launched month\n",
    "* Whether it was a weekend or not\n",
    "* Duration of the campaign \n",
    "\n",
    "Another group of features which is obviously effective is based on the goal and pledged amount. We can extract new features by considering the mean goal in each category and main category to see if a specific campaign is an outlier compared to other campaignes of its field, etc. Below is a list of such features:\n",
    "* Mean of the category's goal\n",
    "* Median of the category's goal\n",
    "* Count of the categoty (to see whether the campaign is relatively popular or not)\n",
    "* Mean of the main category's goal\n",
    "* Median of the main category's goal\n",
    "* Count of the main category\n",
    "* Difference between a campaign's goal and mean of its category\n",
    "* Pledged amount as a percentage of the goal\n",
    "* Pledged amount per backer \n",
    "* Average Pledged amount per category \n",
    "* Difference between pledged amount per backer for a campaign and the average received in that category\n",
    "\n",
    "Another interesting feature could be the country where the campaign is launched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks computes the features listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Let's only keep the successful and failed labels and turn it into a binary classification:\n",
    "df.query(\"state in ['failed', 'successful']\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331672"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    \"\"\"Count the syllables in a word\"\"\"\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVowels2LettersRatio(word):\n",
    "    '''Count ratio between vowels and letters'''\n",
    "    word = str(word)\n",
    "    count = 1  \n",
    "    vowels = 0\n",
    "    for i in word:\n",
    "        if i.isalpha():\n",
    "            count = count + 1\n",
    "            if i in 'aeiou':\n",
    "                vowels = vowels + 1\n",
    "    return ((vowels * 1.0) / count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature extraction for the campaign name\n",
    "\n",
    "df[\"num_syllable\"] = df[\"name\"].apply(lambda x: syllable_count(x))\n",
    "df[\"num_words\"] = df[\"name\"].apply(lambda x: len(x.split()))\n",
    "df[\"num_chars\"] = df[\"name\"].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "df['has_exclaimation_mark'] = int(len(re.findall(r'\\!', str(df.name.str[:]))) != 0)  #presence of ! in the name\n",
    "df['has_question_mark'] = int(len(re.findall(r'\\?', str(df.name.str[:]))) != 0)  #presence of ? in the name\n",
    "df['name_is_upper'] = df.name.str.isupper().astype(float)# if name is uppercase\n",
    "df['name_vowel_ratio'] = df.name.apply(countVowels2LettersRatio)# for each name calculate vowels ratio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## feature extraction for the campaign length and timing\n",
    "df[\"launched_month\"] = df[\"launched\"].dt.month\n",
    "df[\"launched_week\"] = df[\"launched\"].dt.week\n",
    "df[\"launched_day\"] = df[\"launched\"].dt.weekday\n",
    "df[\"is_weekend\"] = df[\"launched_day\"].apply(lambda x: 1 if x > 4 else 0)\n",
    "df[\"duration_in_days\"] = df[\"deadline\"] - df[\"launched\"]\n",
    "df[\"duration_in_days\"] = df[\"duration_in_days\"].apply(lambda x: int(str(x).split()[0]))\n",
    "\n",
    "## label encoding the categorical features. This is necessary for the features related to goal and pledged amounts\n",
    "#df = pd.concat([df, pd.get_dummies(df[\"main_category\"])], axis = 1)\n",
    "label_encoder = LabelEncoder()\n",
    "for cat in [\"category\", \"main_category\"]:\n",
    "    df[cat] = label_encoder.fit_transform(df[cat])\n",
    "   \n",
    "## feature extraction for goal and pledged values\n",
    "df.head()\n",
    "c1 = df.groupby(\"main_category\").agg({\"goal\" : \"mean\", \"category\" : \"sum\"})\n",
    "c2 = df.groupby(\"category\").agg({\"goal\" : \"mean\", \"main_category\" : \"sum\"})\n",
    "c1 = c1.reset_index().rename(columns={\"goal\" : \"main_category_goal_mean\", \"category\" : \"main_category_count\"})\n",
    "c2 = c2.reset_index().rename(columns={\"goal\" : \"category_goal_mean\", \"main_category\" : \"category_count\"})\n",
    "df = df.merge(c2, on = \"category\")\n",
    "df = df.merge(c1, on = \"main_category\") \n",
    "df[\"diff_mean_category_goal\"] = df[\"category_goal_mean\"] - df[\"goal\"]\n",
    "df[\"diff_mean_category_goal\"] = df[\"main_category_goal_mean\"] - df[\"goal\"]\n",
    "df['goal_log'] = np.log1p(df.goal) # normalizing goal by applying log to prevent skewness\n",
    "df.loc[:,'goal_reached'] = df['pledged'] / df['goal'] # Pledged amount as a percentage of goal.\n",
    "df.loc[df['backers'] == 0, 'backers'] = 1 # In backers column, impute 0 with 1 to prevent undefined division.\n",
    "df.loc[:,'pledge_per_backer'] = df['pledged'] / df['backers'] # Pledged amount per backer.\n",
    "\n",
    "## create indicator variable for `country` variable\n",
    "df['country_is_us'] = (df.country == 'US').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"state\"] = df[\"state\"].apply(lambda x: 1 if x==\"successful\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop([\"launched\", \"deadline\", \"name\", \"currency\", \"country\"], axis = 1) #drop the unnecassary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the label and features and scale the data\n",
    "label = df.state\n",
    "features = [c for c in df.columns if c not in [\"state\"]]\n",
    "#Let's normalise the data entries\n",
    "df_scaled = pd.DataFrame(sklearn.preprocessing.normalize(df[features]))\n",
    "#df_scaled.columns=list(df_scaled[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to split the data set to test and train and implement a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "* https://www.kaggle.com/shivamb/an-insightful-story-of-crowdfunding-projects\n",
    "* https://www.kaggle.com/srishti280992/kickstarter-project-classification-lgbm-70-3\n",
    "* https://www.kaggle.com/dronqo/how-successful-is-your-kickstarter-project\n",
    "* https://www.kaggle.com/kromel/kickstarter-successful-vs-failed\n",
    "* https://www.kaggle.com/carlolepelaars/exploration-of-kickstarter-data-2010-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and testing set, use a simple midpoint separation\n",
    "midpoint=int(df.shape[0]/2)\n",
    "df_train = df.iloc[0:midpoint,:]\n",
    "df_test = df.iloc[midpoint:int(df.shape[0]),:]\n",
    "label_train=df.state.iloc[0:midpoint]\n",
    "label_test=df.state.iloc[midpoint:int(df.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the logistic regression model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "#fit and transform the training data\n",
    "X = model_lr.fit(df_train,label_train)\n",
    "# cv_test = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction and apply model to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model_lr.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain an accuracy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.31438288429533\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.98      0.82     99675\n",
      "          1       0.94      0.38      0.54     66161\n",
      "\n",
      "avg / total       0.80      0.74      0.71    165836\n",
      "\n",
      "[[97966  1709]\n",
      " [40887 25274]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(df_test.state,predictions)\n",
    "print (score*100)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test.state,predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(df_test.state,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Random Forest Classifier model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rfc = RandomForestClassifier()\n",
    "\n",
    "#fit and transform the training data\n",
    "X = model_rfc.fit(df_train,label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction and apply model to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model_rfc.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain an accuracy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     99675\n",
      "          1       1.00      1.00      1.00     66161\n",
      "\n",
      "avg / total       1.00      1.00      1.00    165836\n",
      "\n",
      "[[99675     0]\n",
      " [    0 66161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(df_test.state,predictions)\n",
    "print (score*100)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test.state,predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(df_test.state,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Gradient Boost Classifier model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_gbc = GradientBoostingClassifier()\n",
    "\n",
    "#fit and transform the training data\n",
    "X = model_gbc.fit(df_train,label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction and apply model to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model_gbc.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain an accuracy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     99675\n",
      "          1       1.00      1.00      1.00     66161\n",
      "\n",
      "avg / total       1.00      1.00      1.00    165836\n",
      "\n",
      "[[99675     0]\n",
      " [    0 66161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(df_test.state,predictions)\n",
    "print (score*100)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test.state,predictions))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(df_test.state,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
